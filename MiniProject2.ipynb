{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniProject2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiCstLxQjWJD"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder as ohe\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "#import winsound as beep\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def plotRocCurve(data, flag=0):\n",
        "  \n",
        "  category_list = ['rpg', 'rpg_gamers', 'anime', 'hardware', 'cars', 'gamernews', \n",
        "                   'gamedev', 'computers']\n",
        "  \n",
        "  for cat in category_list:\n",
        "\n",
        "    X_train, X_test, y_train, y_test = getXYandTest(data)\n",
        "\n",
        "    for i in range(0, len(y_train)):\n",
        "      if y_train.iloc[i] != cat:\n",
        "        y_train.iloc[i] = 'blah'\n",
        "\n",
        "    for i in range(0, len(y_test)):\n",
        "      if y_test.iloc[i] != cat:\n",
        "        y_test.iloc[i] = 'blah'\n",
        "\n",
        "    # Random/Dummy\n",
        "    if flag == 1:\n",
        "      classifier = DummyClassifier(strategy='uniform')\n",
        "      cat += \" Random\"\n",
        "    \n",
        "    # Regular LinearSVC()\n",
        "    else:\n",
        "      classifier = LinearSVC()\n",
        "    \n",
        "    classifier.fit(X_train, y_train)\n",
        "    print(\"\\n\\nCategory: \", cat)\n",
        "    metrics.plot_roc_curve(classifier, X_test, y_test)\n",
        "    plt.show()\n",
        "\n",
        "def cleanText(text):\n",
        "  text = BeautifulSoup(text, \"lxml\").text\n",
        "  \n",
        "  # Get rid of numbers\n",
        "  newText = ''.join([i for i in text if not i.isdigit()])\n",
        "  newText = newText.replace(\"_\", \" \")\n",
        "  tokens = word_tokenize(newText)\n",
        "  porter = PorterStemmer()\n",
        "\n",
        "  # Turn words into their stem words\n",
        "  stemmed = [porter.stem(word) for word in tokens]\n",
        "  \n",
        "  newText = \"\"\n",
        "  for stemWord in stemmed:\n",
        "    newText += \" \"\n",
        "    newText += stemWord\n",
        "  \n",
        "  #print(\"\\n\\nType newText: \", type(newText))\n",
        "  return newText\n",
        "\n",
        "def getXY(data):\n",
        "  x = data.loc[:, data.columns != 'category']\n",
        "  y = data.category\n",
        "  return x, y\n",
        "\n",
        "def getXYandTest(data):\n",
        "    x, y = getXY(data)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def decisionTree(data):\n",
        "  X_train, X_test, y_train, y_test = getXYandTest(data)\n",
        "  classifier = DecisionTreeClassifier()\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_predicted = classifier.predict(X_test)\n",
        "  print(\"\\n\\nACCURACY DECISION TREE: \", metrics.accuracy_score(y_test, y_predicted))\n",
        "\n",
        "def linearSVC(data, test):\n",
        "  X_train, X_test, y_train, y_test = getXYandTest(data)\n",
        "  classifier = LinearSVC()\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_predicted = classifier.predict(X_test)\n",
        "  print(\"\\n\\nACCURACY Linear SVC Reddit-train: \", metrics.accuracy_score(y_test, y_predicted))\n",
        "\n",
        "  x2, y2 = getXY(data)\n",
        "  classifier2 = LinearSVC()\n",
        "  classifier2.fit(x2, y2)\n",
        "\n",
        "  y2predicted = classifier2.predict(X_test)\n",
        "  print(\"\\n\\nACCURACY Linear SVC TEST: \", metrics.accuracy_score(y_test, y2predicted))\n",
        "  print(\"\\nConfusion Matrix of Linear SVC\\n\", confusion_matrix(y_test, y2predicted))\n",
        "\n",
        "  # Predict reddit-test-data.csv for kaggle submission\n",
        "  X_kaggle_test = test\n",
        "  y_predicted_test = classifier2.predict(X_kaggle_test)\n",
        "  print(\"Y PREDICTED LINEAR SVC\\n\", y_predicted_test)\n",
        "  test_submission_df = pd.DataFrame(y_predicted_test, columns=['subreddit'])\n",
        "  return test_submission_df\n",
        "\n",
        "class NaiveBayes:\n",
        "  def setFrequencies(self, data):\n",
        "    unique_categories = data.groupby('category').sum()\n",
        "    category_counts = data['category'].value_counts()\n",
        "    total_counts = unique_categories.sum()\n",
        "    tot = category_counts.sum()\n",
        "\n",
        "    self.catCounts = category_counts + 2\n",
        "    self.wordCounts = total_counts\n",
        "    self.total = tot\n",
        "    self.uCat = unique_categories + 1\n",
        "\n",
        "  def setTheta(self):\n",
        "    theta = self.catCounts.div(self.total)\n",
        "    self.theta = theta\n",
        "\n",
        "  def setThetaOne(self):\n",
        "    theta = self.uCat.div(self.catCounts, axis=0)\n",
        "    self.thetaOne = theta\n",
        "\n",
        "  def setThetaTwo(self):\n",
        "    theta = self.uCat.sub(self.wordCounts, axis=1)\n",
        "    theta = theta * (-1)\n",
        "    zeroCounts = self.total - self.catCounts\n",
        "    theta = theta.div(zeroCounts, axis=0)\n",
        "    self.thetaTwo = theta\n",
        "\n",
        "  def predict(self, data):\n",
        "    negativeTheta = 1 - self.theta\n",
        "    logTheta = self.theta.div(negativeTheta)\n",
        "    logTheta = np.log(logTheta)\n",
        "    \n",
        "    logThetaPos = self.thetaOne.div(self.thetaTwo)\n",
        "    logThetaPos = np.log(logThetaPos)\n",
        "    logThetaPos = logThetaPos.multiply(data)\n",
        "\n",
        "    logThetaNeg = (1 - self.thetaOne).div((1 - self.thetaTwo))\n",
        "    logThetaNeg = np.log(logThetaNeg)\n",
        "    logThetaNeg = logThetaNeg.multiply(data)\n",
        "\n",
        "    totalTheta = logThetaPos + logThetaNeg\n",
        "    totalTheta = totalTheta.sum(axis=1)\n",
        "\n",
        "    alpha = logTheta + totalTheta\n",
        "    prediction = alpha[alpha == alpha.max(axis=0)]\n",
        "    return alpha.idxmax()\n",
        "\n",
        "# ================================= Main =================================\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  redditTrainDf = pd.read_csv('/content/sample_data/reddit-train.csv', delimiter=',')\n",
        "  redditTestDf = pd.read_csv('/content/sample_data/reddit-test-data.csv', delimiter=',')\n",
        "\n",
        "\n",
        "  # -----------------------------tfIDF-----------------------------\n",
        "  tfIDFwordNumpyArray = redditTrainDf['body'].to_numpy()\n",
        "\n",
        "  # Attempting text cleaning\n",
        "  countRedditPosts = 0\n",
        "  while countRedditPosts < len(tfIDFwordNumpyArray):\n",
        "    tfIDFwordNumpyArray[countRedditPosts] = cleanText(tfIDFwordNumpyArray[countRedditPosts])\n",
        "    countRedditPosts += 1\n",
        "  print(tfIDFwordNumpyArray)\n",
        "  # ------------------------\n",
        "\n",
        "  # Create and fit the tfIDFvectorizer and create the tfIDFencoded_df\n",
        "  tfIDFvectorizer = TfidfVectorizer(max_features=15000, stop_words='english')\n",
        "  tfIDFvectorizer.fit(tfIDFwordNumpyArray)\n",
        "  tfIDFvector = tfIDFvectorizer.transform(tfIDFwordNumpyArray)\n",
        "  tfIDFencodedVector = tfIDFvector.toarray()\n",
        "  tfIDFcategories = redditTrainDf['subreddit'].to_numpy()\n",
        "  tfIDFencoded_df = pd.DataFrame(tfIDFencodedVector)\n",
        "  tfIDFencoded_df['category'] = tfIDFcategories\n",
        "  # ------------------------\n",
        "\n",
        "  # Get the order of the words in the tfIDFvectorizer\n",
        "  count = 0\n",
        "  wordOrder = []\n",
        "  while count < len(tfIDFvectorizer.vocabulary_):\n",
        "    for i in tfIDFvectorizer.vocabulary_.keys():\n",
        "      if tfIDFvectorizer.vocabulary_[i] == count:\n",
        "        wordOrder.append(i)\n",
        "        break\n",
        "    count += 1\n",
        "  print(\"\\n\\ntfIDF word order: \", wordOrder)\n",
        "  # ------------------------\n",
        "\n",
        "  print(\"-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-\\n\\n\\n\") \n",
        "  # tfIDF Naive Bayes ------------------------\n",
        "  #tfIDFnb = NaiveBayes()\n",
        "  #tfIDFnb.setFrequencies(tfIDFencoded_df)\n",
        "  #tfIDFnb.setTheta()\n",
        "  #tfIDFnb.setThetaOne()\n",
        "  #tfIDFnb.setThetaTwo()\n",
        "  \n",
        "  # Encode the data in reddit-test-data.csv with TfIDF Vectorizer \n",
        "  tfIDFwordNumpyArrayTest = redditTestDf['body'].to_numpy()\n",
        "\n",
        "  # Attempting text cleaning on tfIDFwordNumpyArrayTest\n",
        "  countRedditPosts = 0\n",
        "  while countRedditPosts < len(tfIDFwordNumpyArrayTest):\n",
        "    tfIDFwordNumpyArrayTest[countRedditPosts] = cleanText(tfIDFwordNumpyArrayTest[countRedditPosts])\n",
        "    countRedditPosts += 1\n",
        "  print(tfIDFwordNumpyArrayTest)\n",
        "  # ------------------------\n",
        "\n",
        "  tfIDFvectorTest = tfIDFvectorizer.transform(tfIDFwordNumpyArrayTest)\n",
        "  tfIDFencodedVectorTest = tfIDFvectorTest.toarray()\n",
        "  tfIDFencoded_df_reddit_test = pd.DataFrame(tfIDFencodedVectorTest)\n",
        "\n",
        "  # linear SVC test on reddit-train.csv and reddit-test-data.csv\n",
        "  test_submission_df = linearSVC(tfIDFencoded_df, tfIDFencoded_df_reddit_test)\n",
        "  test_submission_df.to_csv('test_submission_df.csv', sep=',', index=True)\n",
        "  print(\"-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-tfIDF-\\n\\n\\n\")\n",
        "  # -----------------------------tfIDF end-----------------------------\n",
        "\n",
        "\n",
        "  # ----------------------------Count Vectorizer----------------------------\n",
        "  wordNumpyArray = redditTrainDf['body'].to_numpy()\n",
        "\n",
        "  # Attempting text cleaning on wordNumpyArray\n",
        "  countRedditPosts = 0\n",
        "  while countRedditPosts < len(wordNumpyArray):\n",
        "    wordNumpyArray[countRedditPosts] = cleanText(wordNumpyArray[countRedditPosts])\n",
        "    countRedditPosts += 1\n",
        "  print(wordNumpyArray)\n",
        "  # ------------------------\n",
        "\n",
        "  # Create and fit the CountVectorizer and create the encoded_df\n",
        "  vectorizer = CountVectorizer(max_features=15000, stop_words='english', binary=True)\n",
        "  vectorizer.fit(wordNumpyArray)\n",
        "  vector = vectorizer.transform(wordNumpyArray)\n",
        "  encodedVector = vector.toarray()\n",
        "  categories = redditTrainDf['subreddit'].to_numpy()\n",
        "  encoded_df = pd.DataFrame(encodedVector)\n",
        "  encoded_df['category'] = categories\n",
        "  # ------------------------\n",
        "\n",
        "  # Get the order of the words in the CountVectorizer\n",
        "  count = 0\n",
        "  wordOrder = []\n",
        "  while count < len(vectorizer.vocabulary_):\n",
        "    for i in vectorizer.vocabulary_.keys():\n",
        "      if vectorizer.vocabulary_[i] == count:\n",
        "        wordOrder.append(i)\n",
        "        break\n",
        "    count += 1\n",
        "  print(\"\\n\\nCountVectorizer word order: \", wordOrder)\n",
        "  # ------------------------\n",
        "\n",
        "  print(\"-Count Vectorizer -Count Vectorizer -Count Vectorizer -Count Vectorizer -Count Vectorizer\\n\\n\\n\")\n",
        "  # CountVectorizer Naive Bayes ------------------------\n",
        "  #nb = NaiveBayes()\n",
        "  #nb.setFrequencies(encoded_df)\n",
        "  #nb.setTheta()\n",
        "  #nb.setThetaOne()\n",
        "  #nb.setThetaTwo()\n",
        "\n",
        "  # Encode the data in reddit-test-data.csv with Count Vectorizer\n",
        "  wordNumpyArrayTest = redditTestDf['body'].to_numpy()\n",
        "\n",
        "  # Attempting text cleaning on wordNumpyArrayTest\n",
        "  countRedditPosts = 0\n",
        "  while countRedditPosts < len(wordNumpyArrayTest):\n",
        "    wordNumpyArrayTest[countRedditPosts] = cleanText(wordNumpyArrayTest[countRedditPosts])\n",
        "    countRedditPosts += 1\n",
        "  print(wordNumpyArrayTest)\n",
        "  # ------------------------\n",
        "\n",
        "  vectorTest = vectorizer.transform(wordNumpyArrayTest)\n",
        "  encodedVectorTest = vectorTest.toarray()\n",
        "  encoded_df_reddit_test = pd.DataFrame(encodedVectorTest)\n",
        "\n",
        "  # linear SVC test on reddit-train.csv and reddit-test-data.csv\n",
        "  linearSVC(encoded_df, encoded_df_reddit_test)\n",
        "  print(\"-Count Vectorizer -Count Vectorizer -Count Vectorizer -Count Vectorizer -Count Vectorizer\\n\\n\\n\")\n",
        "  # ----------------------------Count Vectorizer End----------------------------\n",
        "  \n",
        "\n",
        "  # ------------------------------- ROC curve -------------------------------\n",
        "  plotRocCurve(tfIDFencoded_df)\n",
        "  plotRocCurve(tfIDFencoded_df, 1)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  results = []\n",
        "  for i in range(len(encoded_df_reddit_test)):\n",
        "    results.append(nb.predict(encoded_df_reddit_test.loc[i]))\n",
        "  \n",
        "  resultsDF = pd.DataFrame({'subreddit' : results})\n",
        "  print(results, \"\\n\")\n",
        "  print(resultsDF)\n",
        "  resultsDF.to_csv('/content/sample_data/results.csv', sep=',')\"\"\"\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  USED FOR ITERATIVE TESTING (uses 10-folding)\n",
        "  k = 10\n",
        "  start = 90000\n",
        "  step = 2500\n",
        "  numSamps = 20\n",
        "  end = start + (numSamps * step)\n",
        "\n",
        "  top = 0\n",
        "  topA = 0\n",
        "\n",
        "  print(\"3 fold with shuffle\")\n",
        "\n",
        "  for maxVal in range(start, end, step):\n",
        "    print(\"\\nIteration:\", maxVal)\n",
        "    kFolds = KFold(n_splits=k, random_state=None)\n",
        "    classifier = LinearSVC()\n",
        "    tfIDFvectorizer = TfidfVectorizer(max_features=maxVal, stop_words='english', ngram_range=(1, 3))\n",
        "\n",
        "    tfIDFvectorizer.fit(tfIDFwordNumpyArray)\n",
        "    tfIDFvector = tfIDFvectorizer.transform(tfIDFwordNumpyArray)\n",
        "    tfIDFencodedVector = tfIDFvector.toarray()\n",
        "    tfIDFencoded_df = pd.DataFrame(tfIDFencodedVector)\n",
        "    tfIDFencoded_df['category'] = tfIDFcategories\n",
        "\n",
        "    dataX, dataY = getXY(tfIDFencoded_df)\n",
        "\n",
        "    crossValResults = cross_val_score(classifier, dataX, dataY, cv = kFolds)\n",
        "    accuracy = crossValResults.mean()\n",
        "    if(accuracy>topA):\n",
        "        topA = accuracy\n",
        "        top = maxVal\n",
        "\n",
        "    resultsDF.loc[maxVal-start] = [maxVal] + [accuracy]\n",
        "\n",
        "  print(resultsDF)\n",
        "  print(\"Top: \", top, \" @ \", topA)\n",
        "  #beep.Beep(1000, 440)\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}